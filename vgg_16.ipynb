{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajayogi/Indian-Food-Classification/blob/master/vgg_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq2oMa29b9WU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61df153a-39a0-48fc-aefc-f29e4782e8ff"
      },
      "source": [
        "pip install split-folders"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.2.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF4xK4NUb_Dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "cbdf5b9f-df52-43d0-f37d-5a09106df47e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAeLBss5b_ZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cfa950d-e3f7-4e7d-d70b-15d11f219290"
      },
      "source": [
        "!ls \"gdrive/My Drive/indian food dataset.zip\"\n",
        "!unzip -q \"gdrive/My Drive/indian food dataset.zip\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'gdrive/My Drive/indian food dataset.zip'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thE-ruOZb_nM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import split_folders\n",
        "# Split with a ratio.\n",
        "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "split_folders.ratio('indian food dataset', output=\"final\", seed=1337, ratio=(.8, .2)) # default values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t8lIUrNfx3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Part 1 - Building the CNN\n",
        "\n",
        "# Importing the Keras libraries and packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c2yYKb_fyGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f1be01b2-396b-4c2d-a407-abd91426b19f"
      },
      "source": [
        "\n",
        "# Part 2 - Fitting the CNN to the images\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "training_set = train_datagen.flow_from_directory('final/train',\n",
        "                                                 target_size = (64, 64),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory('final/val',\n",
        "                                            target_size = (64, 64),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7999 images belonging to 20 classes.\n",
            "Found 2000 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFVdjGb3gR5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf443sC4gVcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "include_top=False,\n",
        "input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7qvM2OBgaIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import os\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(20, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEQ7EjfKgjJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiPqa28ygsmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_dir = ''\n",
        "train_dir = os.path.join(base_dir, 'final/train')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'final/val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xTTi0__g5AN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MB8fJg8g_gN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72ecdf06-c41e-475c-babd-bdc26e242344"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7999 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBxY0fA4hje0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c97c684-19ed-462c-d9c9-0a65ca6a3db3"
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(150, 150),\n",
        "                                                  batch_size=32,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 20 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2ArdtArhWGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btz1zCx3hZoO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1128
        },
        "outputId": "c35cdd5f-c35c-4c30-eb81-29c53a24e3ba"
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=250,\n",
        "                              epochs=30,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=60)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "250/250 [==============================] - 79s 318ms/step - loss: 2.1731 - acc: 0.3362 - val_loss: 1.6457 - val_acc: 0.5177\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - 74s 297ms/step - loss: 1.4703 - acc: 0.5527 - val_loss: 1.3339 - val_acc: 0.6239\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 1.1932 - acc: 0.6323 - val_loss: 1.1097 - val_acc: 0.6623\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - 75s 298ms/step - loss: 1.0312 - acc: 0.6842 - val_loss: 1.1377 - val_acc: 0.6749\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - 74s 297ms/step - loss: 0.9107 - acc: 0.7236 - val_loss: 1.0591 - val_acc: 0.7075\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.8257 - acc: 0.7458 - val_loss: 1.0324 - val_acc: 0.7195\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.7469 - acc: 0.7652 - val_loss: 1.1495 - val_acc: 0.6949\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.6836 - acc: 0.7939 - val_loss: 0.9152 - val_acc: 0.7621\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.6313 - acc: 0.8036 - val_loss: 0.9259 - val_acc: 0.7442\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.5892 - acc: 0.8142 - val_loss: 0.9152 - val_acc: 0.7631\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.5550 - acc: 0.8267 - val_loss: 1.1412 - val_acc: 0.7279\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 74s 294ms/step - loss: 0.5100 - acc: 0.8404 - val_loss: 0.8632 - val_acc: 0.7894\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.4631 - acc: 0.8549 - val_loss: 1.1030 - val_acc: 0.7468\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.4371 - acc: 0.8609 - val_loss: 0.8781 - val_acc: 0.7889\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 74s 294ms/step - loss: 0.4098 - acc: 0.8716 - val_loss: 1.0745 - val_acc: 0.7432\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.3796 - acc: 0.8801 - val_loss: 0.8708 - val_acc: 0.8030\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.3612 - acc: 0.8861 - val_loss: 0.9249 - val_acc: 0.7899\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.3410 - acc: 0.8889 - val_loss: 0.9621 - val_acc: 0.7883\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.3143 - acc: 0.8998 - val_loss: 1.0208 - val_acc: 0.7610\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.2919 - acc: 0.9046 - val_loss: 0.9891 - val_acc: 0.7894\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.2761 - acc: 0.9102 - val_loss: 1.1662 - val_acc: 0.7700\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.2717 - acc: 0.9162 - val_loss: 0.9682 - val_acc: 0.7771\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.2416 - acc: 0.9251 - val_loss: 1.0156 - val_acc: 0.8030\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.2474 - acc: 0.9177 - val_loss: 1.1476 - val_acc: 0.7757\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 74s 298ms/step - loss: 0.2179 - acc: 0.9289 - val_loss: 0.9616 - val_acc: 0.7973\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.2160 - acc: 0.9294 - val_loss: 1.3700 - val_acc: 0.7631\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 74s 296ms/step - loss: 0.1987 - acc: 0.9365 - val_loss: 1.2196 - val_acc: 0.7868\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.2005 - acc: 0.9372 - val_loss: 1.3444 - val_acc: 0.7553\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 74s 297ms/step - loss: 0.1928 - acc: 0.9367 - val_loss: 1.1223 - val_acc: 0.8072\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 74s 295ms/step - loss: 0.1833 - acc: 0.9436 - val_loss: 1.1035 - val_acc: 0.7820\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T25z0IXMc6u1",
        "colab_type": "code",
        "outputId": "9b1e1ae1-0b3a-46de-da20-851ec4437188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import itertools\n",
        "import keras\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img \n",
        "from keras.models import Sequential \n",
        "from keras import optimizers\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dropout, Flatten, Dense \n",
        "from keras import applications \n",
        "from keras.utils.np_utils import to_categorical \n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "import math \n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras import applications\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "train_data_dir = 'final/train'\n",
        "validation_data_dir = 'final/val'\n",
        "nb_train_samples = 7984\n",
        "nb_validation_samples = 2000\n",
        "epochs = 30\n",
        "batch_size = 16\n",
        "\n",
        "def save_bottlebeck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "    # build the VGG16 network\n",
        "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "    bottleneck_features_train = model.predict_generator(generator,nb_train_samples // batch_size)\n",
        "    np.save('bottleneck_features_train.npy',bottleneck_features_train)\n",
        "    generator = datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples // batch_size)\n",
        "    np.save('bottleneck_features_validation.npy',\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "\n",
        "def train_top_model():\n",
        "    train_data = np.load('bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "    print(len(train_data))    \n",
        "    validation_data = np.load('bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(train_data, train_labels,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              validation_data=(validation_data, validation_labels))\n",
        "    model.save_weights(top_model_weights_path)\n",
        "    \n",
        "save_bottlebeck_features()\n",
        "train_top_model()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Found 7999 images belonging to 20 classes.\n",
            "Found 2000 images belonging to 20 classes.\n",
            "7984\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1f4ffe0268e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0msave_bottlebeck_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-1f4ffe0268e6>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m               validation_data=(validation_data, validation_labels))\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;31m# using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 check_loss_and_target_compatibility(\n\u001b[0;32m--> 809\u001b[0;31m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 raise ValueError(\n\u001b[1;32m    272\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (7984, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eMiSvUwb9Qv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BteDuGXyzbLt",
        "colab_type": "code",
        "outputId": "6e633e69-ba11-449e-b068-363aedb99790",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "\n",
        "list = os.listdir('final/train/salad') # dir is your directory path\n",
        "number_files = len(list)\n",
        "print(number_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}